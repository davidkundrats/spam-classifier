{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc265e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b3713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/enron1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38116d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of dictionaries to hold data from dataset \n",
    "#Format: [{'filename': file, 'content': content, 'label': 'ham/spam'}]\n",
    "ham = []\n",
    "spam = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3071e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the list using OS walk, return filled lists\n",
    "def list_fill(ham, spam):\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "    #in subdirectory\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='latin1') as f:\n",
    "                content = f.read()\n",
    "                #if in ham, append ham \n",
    "                if 'ham' in root:\n",
    "                    ham.append({'filename': file, 'content': content, 'label': 'ham'})\n",
    "                #if in spam, append spam\n",
    "                elif 'spam' in root:\n",
    "                    spam.append({'filename': file, 'content': content, 'label': 'spam'})\n",
    "    return ham, spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01ddcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take in 2 lists, create 2 dataframes and then merge. Return merged dataframe\n",
    "def create_df(ham, spam): \n",
    "    ham_df = pd.DataFrame(ham)\n",
    "    spam_df = pd.DataFrame(spam)\n",
    "    df = pd.concat([ham_df, spam_df], ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7552d3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                             filename  \\\n",
      "0     1365.2000-06-16.farmer.ham.txt   \n",
      "1     3560.2001-02-09.farmer.ham.txt   \n",
      "2     0877.2000-04-06.farmer.ham.txt   \n",
      "3     2937.2000-11-27.farmer.ham.txt   \n",
      "4     1270.2000-06-07.farmer.ham.txt   \n",
      "...                              ...   \n",
      "5167     1505.2004-07-09.GP.spam.txt   \n",
      "5168     2148.2004-09-13.GP.spam.txt   \n",
      "5169     2406.2004-10-06.GP.spam.txt   \n",
      "5170     1459.2004-06-30.GP.spam.txt   \n",
      "5171     2030.2004-08-30.GP.spam.txt   \n",
      "\n",
      "                                                content label  \n",
      "0     Subject: revised sea robin availabilities effe...   ham  \n",
      "1     Subject: re : january spot purchases - deals n...   ham  \n",
      "2     Subject: re : buyback / deficiency deals works...   ham  \n",
      "3     Subject: king ranch processed volumes at tailg...   ham  \n",
      "4     Subject: confirming requisitions\\nconfirming t...   ham  \n",
      "...                                                 ...   ...  \n",
      "5167  Subject: what she doesnt know sprig bashaw\\ndi...  spam  \n",
      "5168  Subject: want to lose up to 19 % weight . try ...  spam  \n",
      "5169  Subject: buy cheap viagra through us .\\nhi ,\\n...  spam  \n",
      "5170  Subject: viewsonic airpanel vl 50 15 - inch sm...  spam  \n",
      "5171  Subject: entourage , stockmogul newsletter\\nra...  spam  \n",
      "\n",
      "[5172 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "#Create df, verifiy contents\n",
    "ham, spam = list_fill(ham, spam)\n",
    "df = create_df(ham, spam)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38676670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing phase. Filter text to be all lower case, remove punctuation and newline characters.\n",
    "#Tokenize text and remove stopwords. Return filtered tokens.\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "def preprocess(text): \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('\\n', ' ')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #stemmer = PorterStemmer()\n",
    "    #stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc7dc1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject 8 dear friend size 1 order confirmation order shipped january via fedex federal express tracking number 45954036 thank registering userid 56075519 learn make fortune ebay complete turnkey system software videos turorials clk information clilings\n"
     ]
    }
   ],
   "source": [
    "#Assign a new column for preprocessed content\n",
    "df['preprocessed_content'] = df['content'].apply(preprocess)\n",
    "print(df.iloc[4000]['preprocessed_content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9146815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create generic train and predict functions. \n",
    "#Train takes in split training and testing values,and a model. returns fitted model. \n",
    "#Predict takes in X_test, X_train(for vectorization fitting), y_test values and model. Displays prediction for \n",
    "#first 5 iterations. \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def train(X_train, X_test, y_train, y_test, model):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test) \n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    print(f'Model trained. Model score: {model.score(X_test_vectorized, y_test)}')\n",
    "    return model\n",
    "\n",
    "def predict(X_test, X_train, y_test, model): \n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    for pred in range(5): \n",
    "        prediction = model.predict(X_test_vectorized)\n",
    "        print(f'Predicted: {prediction[pred]}')\n",
    "        print(f'Actual: {y_test.iloc[pred]}')\n",
    "        \n",
    "def print_model_score(X_train, X_test, y_test, model): \n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    print(f'Model score: {model.score(X_test_vectorized, y_test)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "092f4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Model score: 0.9536231884057971\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n"
     ]
    }
   ],
   "source": [
    "#Create model (In this case, decision tree)\n",
    "#Establish X and y values. Split using train_test_split. \n",
    "#Assign model to trained model using train method.\n",
    "#Show predictions for first 5 iterations, comparing predictions to actual values.\n",
    "model = DecisionTreeClassifier()\n",
    "X = df['preprocessed_content']\n",
    "y= df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state= 42)\n",
    "model = train(X_train, X_test, y_train, y_test, model)\n",
    "predict(X_test, X_train, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bdbee742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score: 0.9536231884057971\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n",
      "Predicted: ham\n",
      "Actual: ham\n"
     ]
    }
   ],
   "source": [
    "#Verify model on new unseen data\n",
    "datapath_1 = 'dataset/enron2'\n",
    "ham2 = []\n",
    "spam2 = []\n",
    "ham2, spam2 = list_fill(ham1, spam2)\n",
    "df2 = create_df(ham2,spam2)\n",
    "df2['preprocessed_features'] = df['content'].apply(preprocess)\n",
    "X2 = df2['preprocessed_features']\n",
    "y2 = df2['label']\n",
    "X_train2, X_test2, y_train_2, y_test2 = train_test_split(X, y, test_size = .2, random_state = 42)\n",
    "\n",
    "print_model_score(X_train2, X_test2, y_test2, model)\n",
    "predict(X_test2, X_train2, y_test2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ea81f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee998dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
