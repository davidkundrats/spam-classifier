{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc265e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/enron1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38116d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of dictionaries to hold data from dataset \n",
    "#Format: [{'filename': file, 'content': content, 'label': 'ham/spam'}]\n",
    "ham = []\n",
    "spam = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3071e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the list using OS walk, return filled lists\n",
    "def list_fill(ham, spam):\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "    #in subdirectory\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='latin1') as f:\n",
    "                content = f.read()\n",
    "                #if in ham, append ham \n",
    "                if 'ham' in root:\n",
    "                    ham.append({'filename': file, 'content': content, 'label': 'ham'})\n",
    "                #if in spam, append spam\n",
    "                elif 'spam' in root:\n",
    "                    spam.append({'filename': file, 'content': content, 'label': 'spam'})\n",
    "    return ham, spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ddcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take in 2 lists, create 2 dataframes and then merge. Return merged dataframe\n",
    "def create_df(ham, spam): \n",
    "    ham_df = pd.DataFrame(ham)\n",
    "    spam_df = pd.DataFrame(spam)\n",
    "    df = pd.concat([ham_df, spam_df], ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7552d3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                             filename  \\\n",
      "0     1365.2000-06-16.farmer.ham.txt   \n",
      "1     3560.2001-02-09.farmer.ham.txt   \n",
      "2     0877.2000-04-06.farmer.ham.txt   \n",
      "3     2937.2000-11-27.farmer.ham.txt   \n",
      "4     1270.2000-06-07.farmer.ham.txt   \n",
      "...                              ...   \n",
      "5167     1505.2004-07-09.GP.spam.txt   \n",
      "5168     2148.2004-09-13.GP.spam.txt   \n",
      "5169     2406.2004-10-06.GP.spam.txt   \n",
      "5170     1459.2004-06-30.GP.spam.txt   \n",
      "5171     2030.2004-08-30.GP.spam.txt   \n",
      "\n",
      "                                                content label  \n",
      "0     Subject: revised sea robin availabilities effe...   ham  \n",
      "1     Subject: re : january spot purchases - deals n...   ham  \n",
      "2     Subject: re : buyback / deficiency deals works...   ham  \n",
      "3     Subject: king ranch processed volumes at tailg...   ham  \n",
      "4     Subject: confirming requisitions\\nconfirming t...   ham  \n",
      "...                                                 ...   ...  \n",
      "5167  Subject: what she doesnt know sprig bashaw\\ndi...  spam  \n",
      "5168  Subject: want to lose up to 19 % weight . try ...  spam  \n",
      "5169  Subject: buy cheap viagra through us .\\nhi ,\\n...  spam  \n",
      "5170  Subject: viewsonic airpanel vl 50 15 - inch sm...  spam  \n",
      "5171  Subject: entourage , stockmogul newsletter\\nra...  spam  \n",
      "\n",
      "[5172 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "#Create df, verifiy contents\n",
    "ham, spam = list_fill(ham, spam)\n",
    "df = create_df(ham, spam)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38676670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing phase. Filter text to be all lower case, remove punctuation and newline characters.\n",
    "#Tokenize text and remove stopwords. Return filtered tokens.\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "def preprocess(text): \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('\\n', ' ')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    #stemmer = PorterStemmer()\n",
    "    #stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7dc1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject 8 dear friend size 1 order confirmation order shipped january via fedex federal express tracking number 45954036 thank registering userid 56075519 learn make fortune ebay complete turnkey system software videos turorials clk information clilings\n"
     ]
    }
   ],
   "source": [
    "#Assign a new column for preprocessed content\n",
    "df['preprocessed_content'] = df['content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9146815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create generic train and predict functions. \n",
    "#Train takes in split training and testing values,and a model. returns fitted model. \n",
    "#Predict takes in X_test, X_train(for vectorization fitting), y_test values and model. Displays prediction for \n",
    "#first 5 iterations. \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "global vectorizer \n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def train(X_train, X_test, y_train, y_test, model):\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test) \n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    print(f'Model trained. Model testing score: {model.score(X_test_vectorized, y_test)}')\n",
    "    return model\n",
    "\n",
    "def predict(X_test, X_train, y_test, model, input_data): \n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    if input_data == None: \n",
    "        for pred in range(5): \n",
    "            prediction = model.predict(X_test_vectorized)\n",
    "            print(f'Predicted: {prediction[pred]}')\n",
    "            print(f'Actual: {y_test.iloc[pred]}')\n",
    "    else:\n",
    "        input_data_vectorized = vectorizer.transform(input_data)\n",
    "        prediction = model.predict(input_data_vectorized)\n",
    "        print(f'Predicted: {prediction}')\n",
    "\n",
    "def return_vectorized(X_train, X_test): \n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    return X_train_vectorized, X_test_vectorized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092f4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model (In this case, decision tree)\n",
    "#Establish X and y values. Split using train_test_split. \n",
    "#Assign model to trained model using train method.\n",
    "#Show predictions for first 5 iterations, comparing predictions to actual values.\n",
    "#Intuition = Decision tree will be a good choice. \n",
    "X = df['preprocessed_content']\n",
    "y= df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95518dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Model testing score: 0.9710144927536232\n",
      "Model trained. Model testing score: 0.9458937198067633\n",
      "Model trained. Model testing score: 0.9748792270531401\n",
      "Model trained. Model testing score: 0.9845410628019323\n",
      "Best Params for KNeighborsClassifier: {'n_neighbors': 9}, Best training score: 0.9589\n",
      "KNeighborsClassifier F1 score: 0.9460431654676259, Accuracy score: 0.9710144927536232\n",
      "Best Params for DecisionTreeClassifier: {'max_depth': None}, Best training score: 0.9468\n",
      "DecisionTreeClassifier F1 score: 0.9047619047619047, Accuracy score: 0.9458937198067633\n",
      "Best Params for RandomForestClassifier: {'max_depth': None, 'n_estimators': 100}, Best training score: 0.9795\n",
      "RandomForestClassifier F1 score: 0.9563758389261745, Accuracy score: 0.9748792270531401\n",
      "Best Params for MultinomialNB: {'alpha': 0.1}, Best training score: 0.9787\n",
      "MultinomialNB F1 score: 0.9723183391003459, Accuracy score: 0.9845410628019323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Perform CV testing on prospective models\n",
    "param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}\n",
    "param_grid_dt = {'max_depth': [None, 5, 10, 15]}\n",
    "param_grid_rf = {'n_estimators': [50, 100, 150], 'max_depth': [None, 5, 10]}\n",
    "param_grid_nb = {'alpha': [1.0, 0.1, 0.01, 0.001]}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5)\n",
    "grid_dt = GridSearchCV(dt, param_grid_dt, cv=5)\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5)\n",
    "grid_nb = GridSearchCV(nb, param_grid_nb, cv = 5)\n",
    "\n",
    "cv_models = [grid_knn, grid_dt, grid_rf, grid_nb]\n",
    "X_train_vectorized,X_test_vectorized, vectorizier = return_vectorized(X_train, X_test) \n",
    "\n",
    "for model in cv_models: \n",
    "    model = train(X_train, X_test, y_train, y_test, model)    \n",
    "\n",
    "for model in cv_models:\n",
    "    model_name = str(model.estimator).split('(')[0] \n",
    "    best_params = model.best_params_\n",
    "    best_score = model.best_score_\n",
    "    y_pred = model.best_estimator_.predict(X_test_vectorized)\n",
    "    # Cleaning up and printing the output\n",
    "    print(f\"Best Params for {model_name}: {best_params}, Best training score: {best_score:.4f}\")\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "    a_s = accuracy_score(y_test, y_pred)\n",
    "    print(f'{model_name} F1 score: {f1}, Accuracy score: {a_s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "234305bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_nb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdbee742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample spam email: subject exclusive offer limited time deal dear valued customer congratulations youve selected exclusive limitedtime offer incredible deal guarantees massive discounts wide range products including luxury items gadgets act claim prize click link unlock unbeatable savings hurry offer available limited time dont miss opportunity save big sincerely marketing team\n",
      "Predicted: ['spam']\n"
     ]
    }
   ],
   "source": [
    "#Give the model some new data: spam\n",
    "spam_email = '''Subject: Exclusive Offer: Limited Time Deal\n",
    "\n",
    "Dear Valued Customer,\n",
    "\n",
    "Congratulations! You've been selected for an exclusive, limited-time offer. This incredible deal guarantees massive discounts on a wide range of products, including luxury items, gadgets, and more!\n",
    "\n",
    "Act now to claim your prize. Click the link below to unlock these unbeatable savings!\n",
    "\n",
    "Hurry! This offer is available for a limited time only. Don't miss out on this opportunity to save big!\n",
    "\n",
    "Sincerely,\n",
    "The Marketing Team\n",
    "'''\n",
    "spam_email_pre = preprocess(spam_email)\n",
    "print(f'Sample spam email: {spam_email_pre}')\n",
    "predict(X_test, X_train, y_test, model, [spam_email] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ea81f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ham email: subject regarding upcoming meeting dear team hope message finds well writing inform upcoming meeting scheduled next wednesday office premises discussing project updates client presentations planning next quarter meeting agenda related documents shared via internal drive please make sure review meeting productive discussion presence active participation highly appreciated best regards name\n",
      "Predicted: ['ham']\n"
     ]
    }
   ],
   "source": [
    "#Give the model some new data: ham\n",
    "ham_email = '''Subject: Regarding the Upcoming Meeting\n",
    "\n",
    "Dear Team,\n",
    "\n",
    "I hope this message finds you well. I am writing to inform you about the upcoming meeting scheduled for next Wednesday at our office premises. We will be discussing the project updates, client presentations, and planning for the next quarter.\n",
    "\n",
    "The meeting agenda and related documents have been shared via the internal drive. Please make sure to review them before the meeting for a productive discussion.\n",
    "\n",
    "Your presence and active participation are highly appreciated.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]'''\n",
    "ham_email_pre = preprocess(ham_email)\n",
    "print(f'Sample ham email: {ham_email_pre}')\n",
    "predict(X_test, X_train, y_test, model, [ham_email_pre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix: \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X_train_vectorized, X_test_vectorized = return_vectorized(X_train, X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
